{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Top Down Example 2",
      "provenance": [],
      "collapsed_sections": [
        "sHWEmHCD4Zui",
        "CfPQxUJRwz4D"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99c5b7299a2b435a888523fc02f13726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c109e6882a85416cb35ac1e6514f9082",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1c2a74f998d441a49a66f9b06217b25c",
              "IPY_MODEL_ff9c48fe24bd4e1880923f0f7a8df8e2"
            ]
          }
        },
        "c109e6882a85416cb35ac1e6514f9082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c2a74f998d441a49a66f9b06217b25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_082fbef73a974680848b8261ab0b914a",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 215,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_de48bcf3254142919799995172183371"
          }
        },
        "ff9c48fe24bd4e1880923f0f7a8df8e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c9303732e4734016ae9138402e0e5370",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/215 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2733c73c1a854531895e68ff6837713d"
          }
        },
        "082fbef73a974680848b8261ab0b914a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "de48bcf3254142919799995172183371": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9303732e4734016ae9138402e0e5370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2733c73c1a854531895e68ff6837713d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMUdgWmTFWEs"
      },
      "source": [
        "# **Human Action Recognition with MMPose and Spatio-Tempoal Graph Convolutional Network**\n",
        "Yolov3 + Resnet152 + STGCN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHWEmHCD4Zui"
      },
      "source": [
        "# Download the Florence 3D action dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfPQxUJRwz4D"
      },
      "source": [
        "## Florence 3D actions dataset\n",
        "\n",
        "The dataset collected at the University of Florence during 2012, has been captured using a Kinect camera. It includes 9 activities: wave, drink from a bottle, answer phone,clap, tight lace, sit down, stand up, read watch, bow. During acquisition, 10 subjects were asked to perform the above\n",
        "actions for 2/3 times. This resulted in a total of 215 activity samples.\n",
        "We suggest a leave-one-actor-out protocol: train your classifier using all the sequences from 9 out of 10 actors and test on the remaining one. Repeat this procedure for all actors and average the 10 classification accuracy values.\n",
        "\n",
        "Actions \n",
        "1.\twave\n",
        "2.\tdrink from a bottle\n",
        "3.\tanswer phone\n",
        "4.\tclap\n",
        "5.\ttight lace\n",
        "6.\tsit down\n",
        "7.\tstand up\n",
        "8.\tread watch\n",
        "9.\tbow\n",
        "\n",
        "Videos depicting the actions are named GestureRecording_Id\\<ID_GESTURE\\>actor\\<ID_ACTOR\\>idAction\\<ID_ACTION\\>category\\<ID_CATEGORY\\>.avi\n",
        "The file The file Florence_dataset_Features.txt contains all the pose features with annotate actor and actions. Each line is formatted according to the following:\n",
        "\n",
        "%idvideo idactor idcategory  f1....fn\n",
        "\n",
        "where f1-f24 are our normalized body part coordinates\n",
        "and f25 is the normalized frame value.\n",
        "\n",
        "Specifically:  \n",
        "  elbows: f1-f6; (1-3 left elbow, 4-6 right elbow, same applies for all other joints)  \n",
        "  wrists: f13-f18  \n",
        "  knees: f7-f12  \n",
        "  ankles: f19-f24  \n",
        "  normalized frame value: f25  \n",
        "\n",
        "The file Florence_dataset_WorldCoordinates.txt\n",
        "Contains the world coordinates for all the joints. Thanks to Maxime Devanne for parsing this data! Each line is formatted according to the following:\n",
        "\n",
        "%idvideo idactor idcategory  f1....fn\n",
        "where f1-f45 are world coordinates of all the 15 joints.\n",
        "\n",
        "Specifically:  \n",
        "  Head: f1-f3  \n",
        "  Neck: f4-f6  \n",
        "  Spine: f7-f9  \n",
        "  Left Shoulder: f10-f12  \n",
        "  Left Elbow: f13-f15  \n",
        "  Left Wrist: f16-f18  \n",
        "  Right Shoulder: f19-f21  \n",
        "  Right Elbow: f22-f24  \n",
        "  Right Wrist: f25-f27  \n",
        "  Left Hip: f28-f30  \n",
        "  Left Knee: f31-f33  \n",
        "  Left Ankle: f34-f36  \n",
        "  Right Hip: f37-f39  \n",
        "  Right Knee: f40-f42  \n",
        "  Right Ankle: f43-f45  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRabRR67wwqO",
        "outputId": "8dba3db2-702c-4d00-daab-88ef65c0023e"
      },
      "source": [
        "%%shell\n",
        "curl https://www.micc.unifi.it/vim/wp-content/uploads/datasets/florence3d_actions.zip -o florence3d_actions.zip\n",
        "unzip -o -q florence3d_actions.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  303M  100  303M    0     0  16.1M      0  0:00:18  0:00:18 --:--:-- 20.4M\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaUYwXWoreZk"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqrgrMWvaDLZ"
      },
      "source": [
        "import cv2\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "def frame_iter(capture, description = \"\"):\n",
        "  def _iterator():\n",
        "    while capture.grab():\n",
        "      yield capture.retrieve()\n",
        "\n",
        "  return tqdm(\n",
        "    _iterator(),\n",
        "    desc=description,\n",
        "    total=int(capture.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "    leave=False,\n",
        "  )\n",
        "\n",
        "\n",
        "def process_mmdet_results(mmdet_results, cat_id=0):\n",
        "    \"\"\"Process mmdet results, and return a list of bboxes.\n",
        "    :param mmdet_results:\n",
        "    :param cat_id: category id (default: 0 for human)\n",
        "    :return: a list of detected bounding boxes\n",
        "    \"\"\"\n",
        "    if isinstance(mmdet_results, tuple):\n",
        "        det_results = mmdet_results[0]\n",
        "    else:\n",
        "        det_results = mmdet_results\n",
        "\n",
        "    bboxes = det_results[cat_id]\n",
        "\n",
        "    person_results = []\n",
        "    for bbox in bboxes:\n",
        "        person = {}\n",
        "        person['bbox'] = bbox\n",
        "        person_results.append(person)\n",
        "\n",
        "    return person_results\n",
        "\n",
        "\n",
        "# visualization\n",
        "def show_local_mp4_video(file_name, width=640, height=480):\n",
        "  import io\n",
        "  import base64\n",
        "  from IPython.display import HTML\n",
        "  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
        "  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
        "                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
        "                      </video>'''.format(width, height, video_encoded.decode('ascii')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwYPQAlNZkL1"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTQtBhGpewSZ"
      },
      "source": [
        "## Install MMPose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gUtNbQO1qLG",
        "outputId": "64907248-7c66-4ee9-b414-765c0856d320"
      },
      "source": [
        "%%shell\n",
        "pip install -q tqdm poseval@git+https://github.com/svenkreiss/poseval.git\n",
        "# This installation takes a long time due to the compiation of mmcv-full 'about 10mins'\n",
        "pip install -q mmpose mmdet mmcv-full \n",
        "\n",
        "git clone https://github.com/open-mmlab/mmpose.git\n",
        "# cd mmpose\n",
        "# pip install -q -r requirements.txt\n",
        "# python setup.py -q develop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▏                             | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 6.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 6.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n",
            "\u001b[?25h  Building wheel for poseval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pytest-benchmark 3.2.3 has requirement pytest>=3.8, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 256kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 542kB 14.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 44.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 19.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 24.2MB/s \n",
            "\u001b[?25h  Building wheel for mmcv-full (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for chumpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mmpycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'mmpose'...\n",
            "remote: Enumerating objects: 192, done.\u001b[K\n",
            "remote: Counting objects: 100% (192/192), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 6398 (delta 70), reused 103 (delta 34), pack-reused 6206\u001b[K\n",
            "Receiving objects: 100% (6398/6398), 16.29 MiB | 33.49 MiB/s, done.\n",
            "Resolving deltas: 100% (4120/4120), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4BGy7b2LL39"
      },
      "source": [
        "# Pose estimation using TopDown method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwDIh4ePOMnY",
        "outputId": "374fb760-c2ef-45ba-b7b8-a8b30d0cf24f"
      },
      "source": [
        "%%shell\n",
        "cd /content/\n",
        "git clone https://github.com/open-mmlab/mmdetection.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mmdetection'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 15857 (delta 2), reused 4 (delta 1), pack-reused 15831\u001b[K\n",
            "Receiving objects: 100% (15857/15857), 16.91 MiB | 27.27 MiB/s, done.\n",
            "Resolving deltas: 100% (10910/10910), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvHfJ7oeAf3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf1631d-773b-47e7-ac6b-e32877ff176c"
      },
      "source": [
        "%cd /content/mmpose\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import pickle\n",
        "import os.path as osp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mmpose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NisplTDAhL2"
      },
      "source": [
        "## Pose estimation definition\n",
        "PoseResnet152"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUJd1oQz-4yE"
      },
      "source": [
        "from mmdet.apis import inference_detector, init_detector\n",
        "from mmpose.apis import inference_top_down_pose_model, init_pose_model, vis_pose_result\n",
        "\n",
        "\n",
        "def inference_pose_estimation_model(video_path, \n",
        "                            return_heatmap = False, \n",
        "                            save_out_video = True, \n",
        "                            out_video_root = '/content/video_results'):\n",
        "  \n",
        "  det_model = init_detector(\n",
        "        '/content/mmdetection/configs/yolo/yolov3_d53_mstrain-416_273e_coco.py', \n",
        "        'http://download.openmmlab.com/mmdetection/v2.0/yolo/yolov3_d53_mstrain-416_273e_coco/yolov3_d53_mstrain-416_273e_coco-2b60fcd9.pth', \n",
        "        )\n",
        "  \n",
        "  # build the pose model from a config file and a checkpoint file\n",
        "  pose_model = init_pose_model(\n",
        "      '/content/mmpose/configs/top_down/darkpose/coco/hrnet_w48_coco_384x288_dark.py', # model configuration\n",
        "      'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_384x288_dark-741844ba_20200812.pth', # pretrained model\n",
        "      )\n",
        "\n",
        "  dataset = pose_model.cfg.data['test']['type']\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "  if save_out_video:\n",
        "    os.makedirs(out_video_root, exist_ok=True)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "            int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    videoWriter = cv2.VideoWriter(\n",
        "        os.path.join(out_video_root,f'vis_{os.path.basename(video_path)}'), fourcc, fps, size)\n",
        "\n",
        "  # e.g. use ('backbone', ) to return backbone feature\n",
        "  output_layer_names = ()#('backbone', )\n",
        "  results = []\n",
        "  video = frame_iter(cap)\n",
        "  video.set_postfix({'filename': osp.basename(video_path)})\n",
        "  for flag, img in video:\n",
        "    if not flag:\n",
        "      break\n",
        "\n",
        "    # test a single image, the resulting box is (x1, y1, x2, y2)\n",
        "    mmdet_results = inference_detector(det_model, img)\n",
        "\n",
        "    # keep the person class bounding boxes.\n",
        "    person_bboxes = process_mmdet_results(mmdet_results)\n",
        "\n",
        "    # inference the model\n",
        "    pose_results, returned_outputs = inference_top_down_pose_model(\n",
        "      pose_model,\n",
        "      img,\n",
        "      person_bboxes,\n",
        "      bbox_thr=0.3,\n",
        "      format='xyxy',\n",
        "      dataset=dataset,\n",
        "      return_heatmap=return_heatmap,\n",
        "      outputs=output_layer_names)\n",
        "    results.append(pose_results)\n",
        "\n",
        "    if save_out_video:\n",
        "      # show the results\n",
        "      vis_img = vis_pose_result(\n",
        "        pose_model,\n",
        "        img,\n",
        "        pose_results,\n",
        "        dataset=dataset,\n",
        "        kpt_score_thr=0.3,\n",
        "        show=False)\n",
        "      \n",
        "      videoWriter.write(vis_img)\n",
        "\n",
        "  video.close()\n",
        "  cap.release()\n",
        "  if save_out_video:\n",
        "      videoWriter.release()\n",
        "  cv2.destroyAllWindows()\n",
        "  \n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTIOqwl9rPUD"
      },
      "source": [
        "## Run the inference on the Florence 3D action dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOWV2nyhVV0_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "99c5b7299a2b435a888523fc02f13726",
            "c109e6882a85416cb35ac1e6514f9082",
            "1c2a74f998d441a49a66f9b06217b25c",
            "ff9c48fe24bd4e1880923f0f7a8df8e2",
            "082fbef73a974680848b8261ab0b914a",
            "de48bcf3254142919799995172183371",
            "c9303732e4734016ae9138402e0e5370",
            "2733c73c1a854531895e68ff6837713d"
          ]
        },
        "outputId": "054f1a17-a703-4176-caf9-1b9ae7187efe"
      },
      "source": [
        "save_out_video = True\n",
        "\n",
        "video_list = glob.glob('/content/Florence_3d_actions/*.avi')\n",
        "video_list.sort(reverse=True)\n",
        "pose_estimation_results = {}\n",
        "with tqdm(total=len(video_list)) as pbar:\n",
        "  for video_path in video_list:\n",
        "    filename = osp.basename(video_path)\n",
        "    results = inference_pose_estimation_model(video_path, save_out_video = save_out_video)\n",
        "    pose_estimation_results[filename] = results\n",
        "    pbar.update(1)\n",
        "    if save_out_video:\n",
        "      video_result = video_path.replace('Florence_3d_actions/','video_results/vis_')\n",
        "      video_result1 = video_result.replace('avi','mp4')\n",
        "      !ffmpeg -y -loglevel panic -i $video_result -vcodec libx264 $video_result1\n",
        "      !rm $video_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99c5b7299a2b435a888523fc02f13726",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=215.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ta66QACt1b"
      },
      "source": [
        "import random\n",
        "video_result_list = glob.glob('/content/video_results/*.mp4')\n",
        "video_result = random.choice(video_result_list)\n",
        "show_local_mp4_video(video_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8vfgPIBFL_X"
      },
      "source": [
        "print(pose_estimation_results.keys())\n",
        "pickle.dump(pose_estimation_results, open(\"/content/pose_estimation_results_poseresnet152.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnFnyrmCbk9J"
      },
      "source": [
        "# Change current working directory to /content\n",
        "%cd /content\n",
        "!zip -q -r video_results_poseresnet152.zip video_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxd5RBD3Us1e"
      },
      "source": [
        "# Action Recognition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrPT1ecNUs1h"
      },
      "source": [
        "%%shell\n",
        "# cd /content\n",
        "# rm -rf st-gcn-pytorch\n",
        "git clone https://github.com/taznux/st-gcn-pytorch\n",
        "cd st-gcn-pytorch\n",
        "mkdir dataset models\n",
        "ln -sf /content/Florence_3d_actions dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADUMBmeNUs1i"
      },
      "source": [
        "## STGCN Human Action Recognition on Florence 3D skeletion data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPx2iR3VUs1i"
      },
      "source": [
        "%cd /content/st-gcn-pytorch\n",
        "#!python preprocess.py # skeleton conversion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTTbef_FS5j6"
      },
      "source": [
        "import random\n",
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "data = pose_estimation_results\n",
        "\n",
        "train = []\n",
        "valid = []\n",
        "test  = []\n",
        "train_label = []\n",
        "valid_label = []\n",
        "test_label  = []\n",
        "\n",
        "vid = 0\n",
        "for d in data:\n",
        "  ids = list(map(int, re.findall('\\d+', d)))\n",
        "  gid, tid, aid, cid = ids\n",
        "\n",
        "  #print(vid, d)\n",
        "  frames = []\n",
        "  for frame in data[d]:\n",
        "    if len(frame) > 0:\n",
        "      pose = frame[0]\n",
        "      keypoints = np.array(pose['keypoints'])[:17]\n",
        "      frames.append(keypoints)\n",
        "  \n",
        "  if len(frames) >= 32:\n",
        "    frames = random.sample(frames, 32)\n",
        "    frames = torch.from_numpy(np.stack(frames, 0))\n",
        "  else:\n",
        "    frames = np.stack(frames, 0)\n",
        "    xloc = np.arange(frames.shape[0])\n",
        "    new_xloc = np.linspace(0, frames.shape[0], 32)\n",
        "    frames = np.reshape(frames, (frames.shape[0], -1)).transpose()\n",
        "\n",
        "    new_datas = []\n",
        "    for fr in frames:\n",
        "      new_datas.append(np.interp(new_xloc, xloc, fr))\n",
        "    frames = torch.from_numpy(np.stack(new_datas, 0)).t()\n",
        "\n",
        "  frames = frames.view(32, -1, 3)\n",
        "\n",
        "  if tid < 9:\n",
        "    train.append(frames)\n",
        "    train_label.append(cid-1)\n",
        "  elif tid < 10:\n",
        "    valid.append(frames)\n",
        "    valid_label.append(cid-1)\n",
        "  else:\n",
        "    test.append(frames)\n",
        "    test_label.append(cid-1)\n",
        "\n",
        "  vid += 1\n",
        "  \n",
        "train_label = torch.from_numpy(np.asarray(train_label))\n",
        "valid_label = torch.from_numpy(np.asarray(valid_label))\n",
        "test_label  = torch.from_numpy(np.asarray(test_label))\n",
        "\n",
        "torch.save((torch.stack(train, 0), train_label), '/content/st-gcn-pytorch/dataset/train.pkl')\n",
        "torch.save((torch.stack(valid, 0), valid_label), '/content/st-gcn-pytorch/dataset/valid.pkl')\n",
        "torch.save((torch.stack(test, 0),  test_label),  '/content/st-gcn-pytorch/dataset/test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JL7aHn3r4vI"
      },
      "source": [
        "%cd /content/st-gcn-pytorch\n",
        "#!python preprocess.py # skeleton conversion\n",
        "!python main.py --num_epochs 100 --batch_size 4 --val_step 10 --dropout_rate 0.01 --weight_decay 0.0001 # train and test"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}